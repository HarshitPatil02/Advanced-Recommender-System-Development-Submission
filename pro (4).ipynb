{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_WV8NBwJhkq"
      },
      "source": [
        "Task 1: Create a Recommendation system\n",
        "\n",
        "Parse movies.csv and create a content based recommendation system using TfidfVectorizer and cosine_similarity from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcd68t0dr6Vy",
        "outputId": "1df98678-9b43-46d9-f07f-ed7673997643"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "551                Pagemaster, The (1994)\n",
              "650      James and the Giant Peach (1996)\n",
              "12                           Balto (1995)\n",
              "661                      Space Jam (1996)\n",
              "55         Kids of the Round Table (1995)\n",
              "621        All Dogs Go to Heaven 2 (1996)\n",
              "1                          Jumanji (1995)\n",
              "59     Indian in the Cupboard, The (1995)\n",
              "124     NeverEnding Story III, The (1994)\n",
              "986       Escape to Witch Mountain (1975)\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to generate TF-IDF matrix in smaller chunks\n",
        "def generate_tfidf_chunks(data, chunk_size):\n",
        "    tfidf = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "    start = 0\n",
        "    while start < len(data):\n",
        "        end = min(start + chunk_size, len(data))\n",
        "        tfidf_matrix_chunk = tfidf.fit_transform(data[start:end])\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "        yield tfidf_matrix_chunk, feature_names, start, end\n",
        "        start = end\n",
        "\n",
        "# Load movies data\n",
        "movies = pd.read_csv('movies.csv')\n",
        "\n",
        "# Compute cosine similarity\n",
        "def compute_cosine_similarity(data, chunk_size):\n",
        "    cosine_sim_chunks = []\n",
        "    row_indices_chunks = []\n",
        "    feature_names_list = []\n",
        "    for tfidf_matrix_chunk, chunk_feature_names, start, end in generate_tfidf_chunks(data, chunk_size):\n",
        "        cosine_sim_chunk = cosine_similarity(tfidf_matrix_chunk)\n",
        "        cosine_sim_chunks.append(cosine_sim_chunk)\n",
        "        row_indices_chunks.append((start, end))\n",
        "        feature_names_list.extend(chunk_feature_names)\n",
        "    max_len = max(len(chunk) for chunk in cosine_sim_chunks)\n",
        "    # Adjust row indices for each chunk\n",
        "    for i, (start, end) in enumerate(row_indices_chunks):\n",
        "        row_indices_chunks[i] = (start, end, max_len)\n",
        "    # Merge cosine similarity matrices\n",
        "    cosine_sim = np.zeros((sum(end - start for start, end, _ in row_indices_chunks), max_len))\n",
        "    current_row = 0\n",
        "    for cosine_sim_chunk, (_, _, max_len) in zip(cosine_sim_chunks, row_indices_chunks):\n",
        "        rows, cols = cosine_sim_chunk.shape\n",
        "        cosine_sim[current_row:current_row+rows, :cols] = cosine_sim_chunk\n",
        "        current_row += rows\n",
        "    return cosine_sim, feature_names_list, row_indices_chunks\n",
        "\n",
        "cosine_sim, feature_names, row_indices = compute_cosine_similarity(movies['genres'], chunk_size=1000)\n",
        "\n",
        "# Function to recommend movies based on similarity\n",
        "def content_recommendations(title, movies_data, cosine_sim=cosine_sim, feature_names=feature_names, row_indices=row_indices):\n",
        "    idx = movies_data.loc[movies_data['title'] == title].index[0]\n",
        "    for start, end, _ in row_indices:\n",
        "        if idx >= start and idx < end:\n",
        "            adjusted_idx = idx - start\n",
        "            break\n",
        "    sim_scores = list(enumerate(cosine_sim[adjusted_idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return movies_data['title'].iloc[movie_indices]\n",
        "\n",
        "# Example usage:\n",
        "content_recommendations('Toy Story (1995)', movies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK4UjGPDJlZy"
      },
      "source": [
        "Parse ratings.csv and create a collaborative filtering based recommendation system using Surprise or Librecommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kK-TIfw4O43",
        "outputId": "193e0dd0-0245-4cee-b3a2-0268c96e3830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3162992 sha256=2ae6626274be70e488522f6993c3aaa5f2ca215b5648a8bf3d3fe387f763ef27\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DehHk1BO4pCg",
        "outputId": "5ad006f3-c014-43b0-bd0c-1091bc219be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8086\n",
            "1167                            Princess Bride, The (1987)\n",
            "1190                        Godfather: Part II, The (1974)\n",
            "2233            Life Is Beautiful (La Vita è bella) (1997)\n",
            "5508     Spirited Away (Sen to Chihiro no kamikakushi) ...\n",
            "10896                          Little Miss Sunshine (2006)\n",
            "12928    Dear Zachary: A Letter to a Son About His Fath...\n",
            "30506                        Ghost in the Shell 2.0 (2008)\n",
            "49559                                Blue Planet II (2017)\n",
            "58237                                    Twin Peaks (1989)\n",
            "60095                                      Parasite (2019)\n",
            "Name: title, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader\n",
        "from surprise import SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "try:\n",
        "    ratings = pd.read_csv('ratings.csv')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"ParserError: {e}. Attempting to skip problematic lines.\")\n",
        "    ratings = pd.read_csv('ratings.csv', error_bad_lines=False)\n",
        "\n",
        "# Convert ratings to numeric, treating non-numeric values as NaN\n",
        "ratings['rating'] = pd.to_numeric(ratings['rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in rating column\n",
        "ratings.dropna(subset=['rating'], inplace=True)\n",
        "\n",
        "# Define the rating scale\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Create the dataset\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "# Load ratings data\n",
        "ratings = pd.read_csv('ratings.csv')\n",
        "\n",
        "\n",
        "# Choose a collaborative filtering algorithm (SVD in this case)\n",
        "model = SVD()\n",
        "\n",
        "# Train the model\n",
        "model.fit(trainset)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.test(testset)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy.rmse(predictions)\n",
        "\n",
        "# Load movies data\n",
        "movies = pd.read_csv('movies.csv')\n",
        "\n",
        "# Merge ratings and movies data on movieId\n",
        "ratings_with_titles = pd.merge(ratings, movies[['movieId', 'title']], on='movieId')\n",
        "\n",
        "# Function to get recommendations for a given user\n",
        "def get_recommendations_for_user(user_id, model, n=10):\n",
        "    # Get all movie ids\n",
        "    all_movie_ids = ratings['movieId'].unique()\n",
        "    # Get movie ids rated by the user\n",
        "    rated_movie_ids = ratings_with_titles[ratings_with_titles['userId'] == user_id]['movieId']\n",
        "    # Get movie ids not rated by the user\n",
        "    unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "    # Predict ratings for unrated movies\n",
        "    unrated_movies = [(user_id, movie_id, 3) for movie_id in unrated_movie_ids]\n",
        "    predictions = model.test(unrated_movies)\n",
        "    # Sort predictions by estimated rating\n",
        "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
        "    # Get top n recommended movie ids\n",
        "    top_n_movie_ids = [pred.iid for pred in sorted_predictions[:n]]\n",
        "    # Get movie titles corresponding to recommended movie ids\n",
        "    top_n_movie_titles = movies[movies['movieId'].isin(top_n_movie_ids)]['title']\n",
        "    return top_n_movie_titles\n",
        "\n",
        "# Example usage:\n",
        "user_id = 1  # Replace with the desired user id\n",
        "recommendations = get_recommendations_for_user(user_id, model)\n",
        "print(recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObJrpyTJxho"
      },
      "source": [
        "Task 2: Prediction\n",
        "\n",
        "Generate 10 recommendations using:\n",
        "content-based system using the user's historical movie preferences.\n",
        "collaborative filtering system using user’s ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDnmbMmOJfud",
        "outputId": "d23263ee-40d0-4690-a8bb-c2ba41e7f410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-651ab86c6c98>:21: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
            "  ratings_data['timestamp'] = pd.to_datetime(ratings_data['timestamp'], unit='s')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collaborative Filtering Recommendations:\n",
            "1                            Jumanji (1995)\n",
            "2                   Grumpier Old Men (1995)\n",
            "3                  Waiting to Exhale (1995)\n",
            "4        Father of the Bride Part II (1995)\n",
            "5                               Heat (1995)\n",
            "6                            Sabrina (1995)\n",
            "7                       Tom and Huck (1995)\n",
            "8                       Sudden Death (1995)\n",
            "9                          GoldenEye (1995)\n",
            "28162               Jesus liebt mich (2012)\n",
            "Name: title, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# Open the ratings file and skip problematic lines\n",
        "lines = []\n",
        "with open('ratings.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader)  # Skip the header row\n",
        "    for line in reader:\n",
        "        if len(line) == 4:  # Ensure each line has the expected number of fields\n",
        "            lines.append(line)\n",
        "\n",
        "# Create a DataFrame from the remaining lines\n",
        "ratings_data = pd.DataFrame(lines, columns=['userId', 'movieId', 'rating', 'timestamp'])\n",
        "\n",
        "# Convert data types if needed\n",
        "ratings_data['userId'] = ratings_data['userId'].astype(int)\n",
        "ratings_data['movieId'] = ratings_data['movieId'].astype(int)\n",
        "ratings_data['rating'] = ratings_data['rating'].astype(float)\n",
        "ratings_data['timestamp'] = pd.to_datetime(ratings_data['timestamp'], unit='s')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load movies data\n",
        "movies_data = pd.read_csv('movies.csv')\n",
        "\n",
        "from surprise import Dataset, Reader\n",
        "from surprise import SVD\n",
        "\n",
        "# Load ratings data\n",
        "ratings_data = pd.read_csv('ratings.csv')\n",
        "\n",
        "# Define the rating scale\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Create the dataset\n",
        "data = Dataset.load_from_df(ratings_data[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Choose a collaborative filtering algorithm (SVD in this case)\n",
        "model = SVD()\n",
        "\n",
        "# Train the model\n",
        "trainset = data.build_full_trainset()\n",
        "model.fit(trainset)\n",
        "\n",
        "# Function to get collaborative filtering recommendations for a given user\n",
        "def collaborative_filtering_recommendations(user_id, model, movies_data, n=10):\n",
        "    # Get all movie ids\n",
        "    all_movie_ids = movies_data['movieId'].unique()\n",
        "    # Get movie ids rated by the user\n",
        "    rated_movie_ids = ratings_data[ratings_data['userId'] == user_id]['movieId']\n",
        "    # Get movie ids not rated by the user\n",
        "    unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "    # Predict ratings for unrated movies\n",
        "    predictions = [model.predict(user_id, movie_id) for movie_id in unrated_movie_ids]\n",
        "    # Sort predictions by estimated rating\n",
        "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
        "    # Get top n recommended movie ids\n",
        "    top_n_movie_ids = [pred.iid for pred in sorted_predictions[:n]]\n",
        "    # Get movie titles corresponding to recommended movie ids\n",
        "    top_n_movie_titles = movies_data[movies_data['movieId'].isin(top_n_movie_ids)]['title']\n",
        "    return top_n_movie_titles\n",
        "\n",
        "# Example usage:\n",
        "user_id = 3  # Replace with the desired user id\n",
        "collaborative_filtering_recommendations = collaborative_filtering_recommendations(user_id, model, movies_data)\n",
        "print(\"Collaborative Filtering Recommendations:\")\n",
        "print(collaborative_filtering_recommendations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM98pqQ3RSkh",
        "outputId": "81eb16ea-076d-4f13-836d-8a45d485347a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content-based recommendations:\n",
            "['Fargo (1996)', 'Get Shorty (1995)', 'Taxi Driver (1976)', 'Amateur (1994)', 'Kiss of Death (1995)', 'Fresh (1994)', 'Guilty as Sin (1993)', 'Killing Zoe (1994)', 'Perfect World, A (1993)', 'Trial by Jury (1994)']\n",
            "\n",
            "Collaborative filtering recommendations:\n",
            "0                      Toy Story (1995)\n",
            "1                        Jumanji (1995)\n",
            "2               Grumpier Old Men (1995)\n",
            "3              Waiting to Exhale (1995)\n",
            "4    Father of the Bride Part II (1995)\n",
            "5                           Heat (1995)\n",
            "6                        Sabrina (1995)\n",
            "7                   Tom and Huck (1995)\n",
            "8                   Sudden Death (1995)\n",
            "9                      GoldenEye (1995)\n",
            "Name: title, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "# Function to generate TF-IDF matrix in smaller chunks\n",
        "def generate_tfidf_chunks(data, chunk_size):\n",
        "    tfidf = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "    start = 0\n",
        "    while start < len(data):\n",
        "        end = min(start + chunk_size, len(data))\n",
        "        tfidf_matrix_chunk = tfidf.fit_transform(data[start:end])\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "        yield tfidf_matrix_chunk, feature_names, start, end\n",
        "        start = end\n",
        "\n",
        "# Compute cosine similarity\n",
        "def compute_cosine_similarity(data, chunk_size):\n",
        "    cosine_sim_chunks = []\n",
        "    row_indices_chunks = []\n",
        "    feature_names_list = []\n",
        "    for tfidf_matrix_chunk, chunk_feature_names, start, end in generate_tfidf_chunks(data, chunk_size):\n",
        "        cosine_sim_chunk = cosine_similarity(tfidf_matrix_chunk)\n",
        "        cosine_sim_chunks.append(cosine_sim_chunk)\n",
        "        row_indices_chunks.append((start, end))\n",
        "        feature_names_list.extend(chunk_feature_names)\n",
        "    max_len = max(len(chunk) for chunk in cosine_sim_chunks)\n",
        "    # Adjust row indices for each chunk\n",
        "    for i, (start, end) in enumerate(row_indices_chunks):\n",
        "        row_indices_chunks[i] = (start, end, max_len)\n",
        "    # Merge cosine similarity matrices\n",
        "    cosine_sim = np.zeros((sum(end - start for start, end, _ in row_indices_chunks), max_len))\n",
        "    current_row = 0\n",
        "    for cosine_sim_chunk, (_, _, max_len) in zip(cosine_sim_chunks, row_indices_chunks):\n",
        "        rows, cols = cosine_sim_chunk.shape\n",
        "        cosine_sim[current_row:current_row+rows, :cols] = cosine_sim_chunk\n",
        "        current_row += rows\n",
        "    return cosine_sim, feature_names_list, row_indices_chunks\n",
        "\n",
        "# Function to recommend movies based on similarity\n",
        "def content_recommendations(title, movies_data, cosine_sim, feature_names, row_indices):\n",
        "    idx = movies_data.loc[movies_data['title'] == title].index[0]\n",
        "    for start, end, _ in row_indices:\n",
        "        if idx >= start and idx < end:\n",
        "            adjusted_idx = idx - start\n",
        "            break\n",
        "    sim_scores = list(enumerate(cosine_sim[adjusted_idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return movies_data['title'].iloc[movie_indices]\n",
        "\n",
        "# Load movies data\n",
        "movies = pd.read_csv('movies.csv')\n",
        "\n",
        "# Compute cosine similarity for content-based recommendation\n",
        "cosine_sim, feature_names, row_indices = compute_cosine_similarity(movies['genres'], chunk_size=1000)\n",
        "\n",
        "# Load ratings data\n",
        "ratings = pd.read_csv('ratings.csv')\n",
        "\n",
        "# Define the rating scale\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Create the dataset\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Choose a collaborative filtering algorithm (SVD in this case)\n",
        "model = SVD()\n",
        "\n",
        "# Train the model\n",
        "model.fit(trainset)\n",
        "\n",
        "# Load movies data again\n",
        "movies = pd.read_csv('movies.csv')\n",
        "\n",
        "# Merge ratings and movies data on movieId\n",
        "ratings_with_titles = pd.merge(ratings, movies[['movieId', 'title']], on='movieId')\n",
        "\n",
        "# Function to get recommendations for a given user using collaborative filtering\n",
        "def get_collab_filtering_recommendations(user_id, model, n=10):\n",
        "    all_movie_ids = ratings['movieId'].unique()\n",
        "    rated_movie_ids = ratings_with_titles[ratings_with_titles['userId'] == user_id]['movieId']\n",
        "    unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "    unrated_movies = [(user_id, movie_id, 3) for movie_id in unrated_movie_ids]\n",
        "    predictions = model.test(unrated_movies)\n",
        "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
        "    top_n_movie_ids = [pred.iid for pred in sorted_predictions[:n]]\n",
        "    top_n_movie_titles = movies[movies['movieId'].isin(top_n_movie_ids)]['title']\n",
        "    return top_n_movie_titles\n",
        "\n",
        "# Function to get recommendations for a given user using content-based filtering\n",
        "def get_content_based_recommendations(user_ratings, movies_data, cosine_sim, feature_names, row_indices, n=10):\n",
        "    top_rated_movies = user_ratings[user_ratings['rating'] >= 4]['movieId']\n",
        "    recommended_movies = []\n",
        "    for movie_id in top_rated_movies:\n",
        "        movie_title = movies_data[movies_data['movieId'] == movie_id]['title'].iloc[0]\n",
        "        recommendations = content_recommendations(movie_title, movies_data, cosine_sim, feature_names, row_indices)\n",
        "        recommended_movies.extend(recommendations)\n",
        "    return recommended_movies[:n]\n",
        "\n",
        "# Example usage:\n",
        "user_id = 1  # Replace with the desired user id\n",
        "user_ratings = ratings_with_titles[ratings_with_titles['userId'] == user_id]\n",
        "content_based_recommendations = get_content_based_recommendations(user_ratings, movies, cosine_sim, feature_names, row_indices)\n",
        "collab_filtering_recommendations = get_collab_filtering_recommendations(user_id, model)\n",
        "print(\"Content-based recommendations:\")\n",
        "print(content_based_recommendations)\n",
        "print(\"\\nCollaborative filtering recommendations:\")\n",
        "print(collab_filtering_recommendations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qwii0bvMjcU"
      },
      "source": [
        "Task 3: Evaluation\n",
        "\n",
        "Provide Metrics to judge recommendation systems\n",
        "\n",
        "Compare and contrast the the two systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "06UkkWkGMlMS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "58ede20f-28a8-4840-e573-7ebc10bb363b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped line: 332: Expected 4 fields, found 1\n",
            "Skipped line: 44101197921: Expected 4 fields, found 1\n",
            "Skipped line: 1632368879: Expected 4 fields, found 1\n",
            "Skipped line: 17583,217348,97306,3.5,1497173362: Expected 4 fields, found 5\n",
            "Skipped line: 25388,3317,4.0,1171217802,3108,5.0,1341360166: Expected 4 fields, found 7\n",
            "Skipped line: 5,1105433970: Expected 4 fields, found 2\n",
            "Skipped line: 45511,797,5.0,842.5,1527883146: Expected 4 fields, found 5\n",
            "Skipped line: error: Error tokenizing data. C error: Expected 4 fields in line 2802413, saw 5\n",
            "\n",
            "Attempting to skip problematic lines.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 4 fields in line 2802413, saw 5\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-6d2bf739c28c>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mmovies_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mratings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ratings.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Define functions for collaborative filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 2802413, saw 5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the ratings.csv file line by line and handle parsing errors manually\n",
        "ratings_data = []\n",
        "with open('ratings.csv', 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        try:\n",
        "            # Try to split the line into fields\n",
        "            fields = line.strip().split(',')\n",
        "            if len(fields) == 4:\n",
        "                # If there are 4 fields, append the data to the ratings_data list\n",
        "                ratings_data.append(fields)\n",
        "            else:\n",
        "                # If there are not 4 fields, skip the line and print a warning\n",
        "                print(f\"Skipped line: {line.strip()}: Expected 4 fields, found {len(fields)}\")\n",
        "        except Exception as e:\n",
        "            # If there's any other error, skip the line and print the exception\n",
        "            print(f\"Skipped line: {line.strip()}: {e}\")\n",
        "\n",
        "# Convert the ratings_data list to a DataFrame\n",
        "ratings_data = pd.DataFrame(ratings_data[1:], columns=ratings_data[0])\n",
        "\n",
        "# Now you can proceed with the rest of your code using the ratings_data DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = 'ratings.csv'\n",
        "\n",
        "# Load data with error_bad_lines parameter to skip lines with parsing errors\n",
        "try:\n",
        "    ratings_data = pd.read_csv(file_path)\n",
        "except pd.errors.ParserError as e:\n",
        "    # Extract the line number from the exception message\n",
        "    error_message = str(e)\n",
        "    line_number = error_message.split(':')[0].split()[-1]\n",
        "    print(f\"Skipped line: {line_number}: {e}\")\n",
        "    print(f\"Attempting to skip problematic lines.\")\n",
        "    # Initialize an empty DataFrame\n",
        "    ratings_data = pd.DataFrame()\n",
        "\n",
        "# Now you can proceed with the rest of your code using the ratings_data DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = 'ratings.csv'\n",
        "\n",
        "# Read the file line by line, skipping the problematic line\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    lines = [line.strip() for line in lines if len(line.split(',')) == 4]\n",
        "\n",
        "# Write the filtered lines to a new file\n",
        "filtered_file_path = 'filtered_ratings.csv'\n",
        "with open(filtered_file_path, 'w') as file:\n",
        "    file.write('\\n'.join(lines))\n",
        "\n",
        "# Load the filtered data into a DataFrame\n",
        "ratings_data = pd.read_csv(filtered_file_path)\n",
        "\n",
        "# Now you can proceed with the rest of your code using the ratings_data DataFrame\n",
        "\n",
        "# Load data\n",
        "movies_data = pd.read_csv('movies.csv')\n",
        "ratings_data = pd.read_csv('ratings.csv')\n",
        "\n",
        "# Define functions for collaborative filtering\n",
        "def evaluate_collab_filtering(user_id, model, actual_ratings, movies_data):\n",
        "    # Get recommended movies for the user\n",
        "    recommended_movie_ids = model.recommend(user_id)\n",
        "\n",
        "    # Get actual ratings given by the user\n",
        "    actual_ratings = actual_ratings[actual_ratings['userId'] == user_id]\n",
        "\n",
        "    # Filter recommended movies to include only those present in actual ratings\n",
        "    recommended_movie_ids = [movie_id for movie_id in recommended_movie_ids if movie_id in actual_ratings['movieId'].values]\n",
        "\n",
        "    # If no common movies found, return None for metrics\n",
        "    if not recommended_movie_ids:\n",
        "        print(\"No common movies between actual ratings and recommendations.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert actual ratings to binary for evaluation\n",
        "    actual_ratings['rating'] = actual_ratings['rating'].apply(lambda x: 1 if x >= 3.5 else 0)\n",
        "    actual_ratings = actual_ratings.set_index('movieId')\n",
        "\n",
        "    # Get predicted ratings for recommended movies\n",
        "    predicted_ratings = pd.DataFrame({'movieId': recommended_movie_ids, 'predicted_rating': [1]*len(recommended_movie_ids)})\n",
        "    predicted_ratings = predicted_ratings.set_index('movieId')\n",
        "\n",
        "    # Merge actual and predicted ratings\n",
        "    merged_ratings = actual_ratings.join(predicted_ratings, how='outer').fillna(0)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    precision = precision_score(merged_ratings['rating'], merged_ratings['predicted_rating'])\n",
        "    recall = recall_score(merged_ratings['rating'], merged_ratings['predicted_rating'])\n",
        "    f1 = f1_score(merged_ratings['rating'], merged_ratings['predicted_rating'])\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Define functions for content-based filtering\n",
        "def evaluate_content_based(user_id, actual_ratings, movies_data, cosine_sim, feature_names, row_indices, n=10):\n",
        "    # Get recommended movies for the user\n",
        "    recommended_movie_ids = content_based_recommendation(user_id, cosine_sim, feature_names, row_indices, n)\n",
        "\n",
        "    # Get actual ratings given by the user\n",
        "    actual_ratings = actual_ratings[actual_ratings['userId'] == user_id]\n",
        "\n",
        "    # Filter recommended movies to include only those present in actual ratings\n",
        "    recommended_movie_ids = [movie_id for movie_id in recommended_movie_ids if movie_id in actual_ratings['movieId'].values]\n",
        "\n",
        "    # If no common movies found, return None for metrics\n",
        "    if not recommended_movie_ids:\n",
        "        print(\"No common movies between actual ratings and recommendations.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert actual ratings to binary for evaluation\n",
        "    actual_ratings['rating'] = actual_ratings['rating'].apply(lambda x: 1 if x >= 3.5 else 0)\n",
        "    actual_ratings = actual_ratings.set_index('movieId')\n",
        "\n",
        "    # Get predicted ratings for recommended movies\n",
        "    predicted_ratings = pd.DataFrame({'movieId': recommended_movie_ids, 'predicted_rating': [1]*len(recommended_movie_ids)})\n",
        "    predicted_ratings = predicted_ratings.set_index('movieId')\n",
        "\n",
        "    # Merge actual and predicted ratings\n",
        "    merged_ratings = actual_ratings.join(predicted_ratings, how='outer').fillna(0)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    precision = precision_score(merged_ratings['rating'], merged_ratings['predicted_rating'])\n",
        "    recall = recall_score(merged_ratings['rating'], merged_ratings['predicted_rating'])\n",
        "    f1 = f1_score(merged_ratings['rating'], merged_ratings['predicted_rating'])\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Evaluate collaborative filtering\n",
        "user_id = 1  # Example user ID\n",
        "model = CollaborativeFilteringModel()  # Assuming you have a collaborative filtering model\n",
        "actual_ratings = ratings_data[['userId', 'movieId', 'rating']]  # Assuming ratings_data has columns 'userId', 'movieId', 'rating'\n",
        "collab_precision, collab_recall, collab_f1 = evaluate_collab_filtering(user_id, model, actual_ratings, movies_data)\n",
        "print(\"Collaborative Filtering Metrics:\")\n",
        "print(\"Precision:\", collab_precision)\n",
        "print(\"Recall:\", collab_recall)\n",
        "print(\"F1-score:\", collab_f1)\n",
        "\n",
        "# Evaluate content-based filtering\n",
        "user_id = 1  # Example user ID\n",
        "cosine_sim = compute_cosine_similarity(tfidf_matrix)  # Assuming you have computed cosine similarity matrix\n",
        "feature_names = tfidf_vectorizer.get_feature_names()  # Assuming you have tfidf_vectorizer\n",
        "row_indices = movies_data['movieId'].values  # Assuming you have row indices for movies in cosine similarity matrix\n",
        "content_precision, content_recall, content_f1 = evaluate_content_based(user_id, actual_ratings, movies_data, cosine_sim, feature_names, row_indices)\n",
        "print(\"\\nContent-based Filtering Metrics:\")\n",
        "print(\"Precision:\", content_precision)\n",
        "print(\"Recall:\", content_recall)\n",
        "print(\"F1-score:\", content_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Define collaborative filtering recommendation system\n",
        "def collaborative_filtering(user_id, model, actual_ratings, movies_data):\n",
        "    all_movie_ids = ratings['movieId'].unique()\n",
        "    rated_movie_ids = ratings_with_titles[ratings_with_titles['userId'] == user_id]['movieId']\n",
        "    unrated_movie_ids = list(set(all_movie_ids) - set(rated_movie_ids))\n",
        "    unrated_movies = [(user_id, movie_id, 3) for movie_id in unrated_movie_ids]\n",
        "    predictions = model.test(unrated_movies)\n",
        "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
        "    top_n_movie_ids = [pred.iid for pred in sorted_predictions[:n]]\n",
        "    top_n_movie_titles = movies[movies['movieId'].isin(top_n_movie_ids)]['title']\n",
        "    pass\n",
        "\n",
        "# Define content-based filtering recommendation system\n",
        "def content_based_filtering(user_id, actual_ratings, movies_data, cosine_sim, feature_names, row_indices):\n",
        "    # Placeholder for content-based filtering implementation\n",
        "    top_rated_movies = user_ratings[user_ratings['rating'] >= 4]['movieId']\n",
        "    recommended_movies = []\n",
        "    for movie_id in top_rated_movies:\n",
        "        movie_title = movies_data[movies_data['movieId'] == movie_id]['title'].iloc[0]\n",
        "        recommendations = content_recommendations(movie_title, movies_data, cosine_sim, feature_names, row_indices)\n",
        "        recommended_movies.extend(recommendations)\n",
        "    pass\n",
        "\n",
        "# Define evaluation function for recommendation systems\n",
        "def evaluate_recommendation_system(actual_ratings, recommended_movies):\n",
        "    \"\"\"\n",
        "    Evaluate a recommendation system using precision, recall, and F1-score.\n",
        "\n",
        "    Args:\n",
        "    actual_ratings (list): List of actual ratings given by the user.\n",
        "    recommended_movies (list): List of recommended movies by the system.\n",
        "\n",
        "    Returns:\n",
        "    precision (float): Precision score.\n",
        "    recall (float): Recall score.\n",
        "    f1 (float): F1-score.\n",
        "    \"\"\"\n",
        "    # Convert recommended movies to a binary array\n",
        "    recommended_binary = np.isin(actual_ratings, recommended_movies).astype(int)\n",
        "\n",
        "    # Precision\n",
        "    precision = precision_score(actual_ratings, recommended_binary)\n",
        "\n",
        "    # Recall\n",
        "    recall = recall_score(actual_ratings, recommended_binary)\n",
        "\n",
        "    # F1-score\n",
        "    f1 = f1_score(actual_ratings, recommended_binary)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Load data\n",
        "movies_data = pd.read_csv('movies.csv')\n",
        "\n",
        "# Load ratings data\n",
        "try:\n",
        "    ratings_data = pd.read_csv('ratings.csv')\n",
        "except pd.errors.ParserError as e:\n",
        "    if len(str(e).split(':')) >= 3:\n",
        "        print(f\"Skipped line: {str(e).split(':')[1].strip()}: {str(e).split(':')[2].strip()}\")\n",
        "    else:\n",
        "        print(f\"Skipped line: {str(e)}\")\n",
        "    print(f\"Attempting to skip problematic lines.\")\n",
        "    # Initialize an empty DataFrame\n",
        "    ratings_data = pd.DataFrame()\n",
        "\n",
        "# Define user ID and actual ratings\n",
        "user_id = 1\n",
        "if 'userId' in ratings_data:\n",
        "    actual_ratings = ratings_data[ratings_data['userId'] == user_id]['movieId'].tolist()\n",
        "else:\n",
        "    print(\"Error: 'userId' column not found in ratings data.\")\n",
        "    actual_ratings = []\n",
        "\n",
        "# Define collaborative filtering model (example)\n",
        "model = None\n",
        "\n",
        "# Evaluate collaborative filtering\n",
        "collab_precision, collab_recall, collab_f1 = evaluate_recommendation_system(actual_ratings, collaborative_filtering(user_id, model, actual_ratings, movies_data))\n",
        "print(\"Collaborative Filtering Metrics:\")\n",
        "print(\"Precision:\", collab_precision)\n",
        "print(\"Recall:\", collab_recall)\n",
        "print(\"F1-score:\", collab_f1)\n",
        "\n",
        "# Define content-based filtering parameters (example)\n",
        "cosine_sim = None\n",
        "feature_names = None\n",
        "row_indices = None\n",
        "\n",
        "# Evaluate content-based filtering\n",
        "content_precision, content_recall, content_f1 = evaluate_recommendation_system(actual_ratings, content_based_filtering(user_id, actual_ratings, movies_data, cosine_sim, feature_names, row_indices))\n",
        "print(\"\\nContent-based Filtering Metrics:\")\n",
        "print(\"Precision:\", content_precision)\n",
        "print(\"Recall:\", content_recall)\n",
        "print(\"F1-score:\", content_f1)\n",
        "\n",
        "# Compare the two recommendation systems\n",
        "print(\"\\nComparison:\")\n",
        "print(\"Collaborative Filtering vs Content-based Filtering\")\n",
        "print(\"Precision:\", collab_precision, \"vs\", content_precision)\n",
        "print(\"Recall:\", collab_recall, \"vs\", content_recall)\n",
        "print(\"F1-score:\", collab_f1, \"vs\", content_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "JcdvTkDtaVyo",
        "outputId": "aa226e5c-5496-4c30-b7c3-d962cda766ef"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped line: Error tokenizing data. C error: Expected 4 fields in line 2802413, saw 5\n",
            "\n",
            "Attempting to skip problematic lines.\n",
            "Error: 'userId' column not found in ratings data.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'test'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-178d2dcd44a7>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Evaluate collaborative filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mcollab_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollab_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollab_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_recommendation_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollaborative_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovies_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Collaborative Filtering Metrics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollab_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-178d2dcd44a7>\u001b[0m in \u001b[0;36mcollaborative_filtering\u001b[0;34m(user_id, model, actual_ratings, movies_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0munrated_movie_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_movie_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrated_movie_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0munrated_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmovie_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munrated_movie_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munrated_movies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msorted_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtop_n_movie_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'test'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}